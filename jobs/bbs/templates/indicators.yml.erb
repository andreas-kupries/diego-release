---
apiVersion: indicatorprotocol.io/v1
kind: IndicatorDocument

metadata:
  labels:
    deployment: <%= spec.deployment %>
    component: bbs

spec:
  product:
    name: diego
    version: latest

  indicators:
  - name: bbs_master_elected
    promql: BBSMasterElected{source_id="bbs"}
    type: kpi
    presentation:
      chartType: bar
    documentation:
      title: BBS - Master Elected
      description: |
        Indicates when there is a BBS master election, meaning that a BBS instance has taken over as the active instance. A value of 1 is emitted when the election takes place.

        Use: This metric will be emitted when a redeployment of the BBS occurs. If this metric is emitted frequently outside of a deployment, this may be a signal of underlying problems that should be investigated. If the active BBS is continually changing, this can cause application push downtime.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: On event
      recommended_alert_thresholds: Dynamic
      recommended_response: |
        1. Check the BBS logs.
        1. Check the BBS VM load average and instance group size.
        1. Check the health of the connection to the backing SQL database and network latency.

  - name: convergence_lrp_duration
    promql: max_over_time(ConvergenceLRPDuration{source_id="bbs"}[15m]) / 1000000000
    type: kpi
    documentation:
      title: BBS - Time to Run LRP Convergence
      description: |
        Time in ns that the BBS took to run its LRP convergence pass.

        Use: If the convergence run begins taking too long, apps or Tasks may be crashing without restarting. This symptom can also indicate loss of connectivity to the BBS database.

        Origin: Firehose
        Type: Gauge (Integer in ns)
        Frequency: 30 s
      recommended_alert_thresholds: |
          Yellow warning: ≥ 10 s
          Red critical: ≥ 20 s
      recommended_response: |
        1. Check BBS logs for errors.
        2. Try vertically scaling the BBS VM resources up. For example, add more CPUs or memory depending on its system.cpu/system.memory metrics.
        3. Consider vertically scaling the backing database, if system.cpu and system.memory metrics for the database instances are high.

  - name: convergence_lrp_runs
    promql: ConvergenceLRPRuns{source_id="bbs"}
    type: other
    documentation:
      title: BBS - LRP Runs
      description: |
        Cumulative number of times BBS has run its LRP convergence pass.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 15 s

  - name: convergence_task_duration
    promql: max_over_time(ConvergenceTaskDuration{source_id="bbs"}[15m]) / 1000000000
    type: kpi
    documentation:
      title: BBS - Time to Run Task Convergence
      description: |
        Time in ns that the BBS took to run its Task convergence pass.

        Use: If the convergence run begins taking too long, apps or Tasks may be crashing without restarting. This symptom can also indicate loss of connectivity to the BBS database.

        Origin: Firehose
        Type: Gauge (Integer in ns)
        Frequency: 30 s
      recommended_alert_thresholds: |
          Yellow warning: ≥ 10 s
          Red critical: ≥ 20 s
      recommended_response: |
        1. Check BBS logs for errors.
        2. Try vertically scaling the BBS VM resources up. For example, add more CPUs or memory depending on its system.cpu/system.memory metrics.
        3. Consider vertically scaling the backing database, if system.cpu and system.memory metrics for the database instances are high.

  - name: convergence_task_runs
    promql: ConvergenceTaskRuns{source_id="bbs"}
    type: other
    documentation:
      title: BBS - Task Runs
      description: |
        Cumulative number of times the BBS has run its Task convergence pass.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 15 s

  - name: convergence_task_kicked
    promql: ConvergenceTaskKicked{source_id="bbs"}
    type: other
    documentation:
      title: BBS - Task Kicked
      description: |
        Cumulative number of times the BBS has updated a Task during its Task convergence pass.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 15 s

  - name: convergence_task_pruned
    promql: ConvergenceTaskPruned{source_id="bbs"}
    type: other
    documentation:
      title: BBS - Task Pruned
      description: |
        Cumulative number of times the BBS has deleted a malformed Task Definition during its Task convergence pass.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 15 s

  - name: crashed_actual_lrps
    promql: avg_over_time(CrashedActualLRPs{source_id="bbs"}[5m])
    type: kpi
    documentation:
      title: BBS - Crashed App Instances
      description: |
        Total number of LRP instances that have crashed.

        Use: Indicates how many instances in the deployment are in a crashed state. An increase in bbs.CrashedActualLRPs can indicate several problems, from a bad app with many instances associated, to a platform issue that is resulting in app crashes. Use this metric to help create a baseline for your deployment. After you have a baseline, you can create a deployment-specific alert to notify of a spike in crashes above the trend line. Tune alert values to your deployment.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: 30 s
      recommended_alert_thresholds: Dynamic
      recommended_response: |
        1. Look at the BBS logs for apps that are crashing and at the cell logs to see if the problem is with the apps themselves, rather than a platform issue.
        1. Alerting thresholds for this metric will vary depending on your deployment’s size and utilization. Monitoring crashed LRPs and investigating their cause (a troublesome application or a problem with the deployment itself) can help determine what levels to set for alerts.

  - name: crashing_desired_lrps
    promql: avg_over_time(CrashingDesiredLRPs{source_id="bbs"}[5m])
    type: kpi
    documentation:
      title: BBS - Crashing Apps
      description: |
        Total number of DesiredLRPs that have at least one crashed instance.

        Use: Indicates how many apps in the deployment have crashed instance. An increase in bbs.CrashingDesiredLRPs can indicate several problems, from a many bad apps, to a platform issue that is resulting in app crashes. Use this metric to help create a baseline for your deployment. After you have a baseline, you can create a deployment-specific alert to notify of a spike in crashes above the trend line. Tune alert values to your deployment.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: 30 s
      recommended_alert_thresholds: Dynamic
      recommended_response: |
        1. Look at the BBS logs for apps that are crashing and at the cell logs to see if the problem is with the apps themselves, rather than a platform issue.
        1. Alerting thresholds for this metric will vary depending on your deployment’s size and utilization. Monitoring crashing LRPs and investigating their cause (a troublesome application or a problem with the deployment itself) can help determine what levels to set for alerts.

  - name: bbs_db_open_connections
    promql: DBOpenConnections{source_id="bbs"}
    type: other
    documentation:
      title: BBS - DB Open Connections
      description: |
        Number of open connections to the SQL database.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 60 s

  - name: bbs_db_queries_failed
    promql: rate(DBQueriesFailed{source_id="bbs"}[5m]) * 60
    type: kpi
    documentation:
      title: BBS - DB Failed Queries
      description: |
        Cumulative number of SQL queries that failed.

        Use: If the number is going up the state changes in LRPs and Tasks are not recorded in BBS database. This symptom can also indicate slow connectivity to the BBS database.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 60 s
      recommended_alert_thresholds: Dynamic
      recommended_response: |
        1. Check BBS logs for errors.
        2. Try vertically scaling the BBS VM resources up. For example, add more CPUs or memory depending on its system.cpu/system.memory metrics.
        3. Consider vertically scaling the backing database, if system.cpu and system.memory metrics for the database instances are high.

  - name: bbs_db_queries_in_flight
    promql: DBQueriesInFlight{source_id="bbs"}
    type: other
    documentation:
      title: BBS - DB Queries In Flight
      description: |
        Maximum number of concurrent in flight queries in the last 60 seconds.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 60 s

  - name: bbs_db_queries_total
    promql: DBQueriesTotal{source_id="bbs"}
    type: other
    documentation:
      title: BBS - DB Queries Total
      description: |
        Cumulative number of SQL queries executed, including BEGIN, COMMIT, and ROLLBACK statements.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 60 s

  - name: bbs_db_queries_succeeded
    promql: DBQueriesSucceeded{source_id="bbs"}
    type: other
    documentation:
      title: BBS - DB Queries Succeeded
      description: |
        Cumulative number of SQL queries that finished successfully.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 60 s

  - name: bbs_db_query_duration
    promql: max_over_time(DBQueryDurationMax{source_id="bbs"}[15m]) / 1000000000
    type: kpi
    documentation:
      title: BBS - DB Query Duration
      description: |
        Maximum duration of all queries that have run in the last 60 seconds.

        Use: If the DB query begins taking too long, the state changes in LRPs and Tasks might be failing to be recorded in BBS database. This symptom can also indicate slow connectivity to the BBS database.

        Origin: Firehose
        Type: Gauge (Integer in ns)
        Frequency: 60 s
      recommended_alert_thresholds: |
          Yellow warning: ≥ 10 s
          Red critical: ≥ 20 s
      recommended_response: |
        1. Check BBS logs for errors.
        2. Try vertically scaling the BBS VM resources up. For example, add more CPUs or memory depending on its system.cpu/system.memory metrics.

  - name: bbs_db_wait_duration
    promql: avg_over_time(DBWaitDuration{source_id="bbs"}[15m]) / 1000000000
    type: kpi
    documentation:
      title: BBS - DB Wait Duration
      description: |
        The total time blocked waiting for a new connection.

        Use: If the DB wait duration is going up the state changes in LRPs and Tasks might be failing to be recorded in BBS database. This symptom can also indicate slow connectivity to the BBS database.

        Origin: Firehose
        Type: Gauge (Integer in ns)
        Frequency: 60 s
      recommended_alert_thresholds: |
          Yellow warning: ≥ 10 s
          Red critical: ≥ 20 s
      recommended_response: |
        1. Check BBS logs for errors.
        2. Try vertically scaling the BBS VM resources up. For example, add more CPUs or memory depending on its system.cpu/system.memory metrics.
        3. Check on the connection metrics emitted by the backing database server.
        4. Consider vertically scaling the backing database, if system.cpu and system.memory metrics for the database instances are high.

  - name: bbs_db_wait_count
    promql: rate(DBWaitCount{source_id="bbs"}[5m]) * 60
    type: kpi
    documentation:
      title: BBS - DB Wait Count
      description: |
        The total time blocked waiting for a new connection.

        Use: If the DB wait count is going up the state changes in LRPs and Tasks might be failing to be recorded in BBS database. This symptom can also indicate slow connectivity to the BBS database.

        Origin: Firehose
        Type: Counter (Integer)
        Frequency: 60 s
      recommended_alert_thresholds: Dynamic
      recommended_response: |
        1. Check BBS logs for errors.
        2. Try vertically scaling the BBS VM resources up. For example, add more CPUs or memory depending on its system.cpu/system.memory metrics.
        3. Check on the connection metrics emitted by the backing database server.
        4. Consider vertically scaling the backing database, if system.cpu and system.memory metrics for the database instances are high.

  - name: domain_cf_apps
    promql: max_over_time(Domain_cf_apps{source_id="bbs"}[5m])
    documentation:
      title: BBS - Cloud Controller and Diego in Sync
      description: |
        Indicates if the cf-apps Domain is up-to-date, meaning that CF App requests from Cloud Controller are synchronized to bbs.LRPsDesired (Diego-desired AIs) for execution.
        - 1 means cf-apps Domain is up-to-date
        - No data received means cf-apps Domain is not up-to-date

        Use: If the cf-apps Domain does not stay up-to-date, changes requested in the Cloud Controller are not guaranteed to propagate throughout the system. If the Cloud Controller and Diego are out of sync, then apps running could vary from those desired.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: 30 s
      recommended_response: |
        1. Check the BBS and Clock Global (Cloud Controller clock) logs.

  - name: encryption_duration
    promql: avg_over_time(EncryptionDuration{source_id="bbs"}[15m]) / 1000000000
    type: other
    documentation:
      title: BBS - Encryption Duration
      description: |
        Time the BBS took to ensure all BBS records are encrypted with the current active encryption key.

        Origin: Firehose
        Type: Gauge (Integer in ns)
        Frequency: During event, when BBS becomes the active master.

  - name: lrps_claimed
    promql: avg_over_time(LRPsClaimed{source_id="bbs"}[5m])
    type: other
    documentation:
      title: BBS - Claimed LRPs
      description: |
        Total number of LRP instances that have been claimed by some cell.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: 30 s

  - name: lrps_desired
    promql: avg_over_time(LRPsDesired{source_id="bbs"}[5m])
    type: other
    documentation:
      title: BBS - Desired LRPs
      description: |
        Total number of LRP instances desired across all LRPs.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: 30 s

  - name: lrps_extra
    promql: avg_over_time(LRPsExtra{source_id="bbs"}[5m])
    type: kpi
    documentation:
      title: BBS - More App Instances Than Expected
      description: |
        Total number of LRP instances that are no longer desired but still have a BBS record. When Diego wants to add more apps, the BBS sends a request to the auctioneer to spin up additional LRPs. LRPsExtra is the total number of LRP instances that are no longer desired but still have a BBS record.

        Use: If Diego has more LRPs running than expected, there may be problems with the BBS.

        Deleting an app with many instances can temporarily spike this metric. However, a sustained spike in bbs.LRPsExtra is unusual and should be investigated.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: 30 s
      recommended_alert_thresholds: |
        Yellow warning: ≥ 5
        Red critical: ≥ 10
      recommended_response: |
        1. Review the BBS logs for proper operation or errors, looking for detailed error messages.
        2. Check the Domain freshness.

  - name: lrps_missing
    promql: avg_over_time(LRPsMissing{source_id="bbs"}[5m])
    type: kpi
    documentation:
      title: BBS - Fewer App Instances Than Expected
      description: |
        Total number of LRP instances that are desired but have no record in the BBS. When Diego wants to add more apps, the BBS sends a request to the auctioneer to spin up additional LRPs. LRPsMissing is the total number of LRP instances that are desired but have no BBS record.

        Use: If Diego has less LRP running than expected, there may be problems with the BBS.

        An app push with many instances can temporarily spike this metric. However, a sustained spike in bbs.LRPsMissing is unusual and should be investigated.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: 30 s
      recommended_alert_thresholds: |
        Yellow warning: ≥ 5
        Red critical: ≥ 10
      recommended_response: |
        1. Review the BBS logs for proper operation or errors, looking for detailed error messages.
        2. Check the Domain freshness.

  - name: lrps_running
    promql: avg_over_time(LRPsRunning{source_id="bbs"}[1h]) - avg_over_time(LRPsRunning{source_id="bbs"}[1h]  offset 1h)
    type: kpi
    documentation:
      title: BBS - Running App Instances, Rate of Change
      description: |
        Rate of change in app instances being started or stopped on the platform. It is derived from bbs.LRPsRunning and represents the total number of LRP instances that are running on Diego cells.

        Use: Delta reflects upward or downward trend for app instances started or stopped. Helps to provide a picture of the overall growth trend of the environment for capacity planning. You may want to alert on delta values outside of the expected range.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: During event, emission should be constant on a running deployment.
      recommended_alert_thresholds: Dynamic
      recommended_response: |
        1. Scale components as necessary.

  - name: lrps_unclaimed
    promql: avg_over_time(LRPsClaimed{source_id="bbs"}[5m])
    type: other
    documentation:
      title: BBS - Unclaimed LRPs
      description: |
        Total number of LRP instances that have not yet been claimed by a cell.

        Origin: Firehose
        Type: Gauge (Float)
        Frequency: 30 s

  - name: bbs_lock_held
    promql: max_over_time(LockHeld{source_id="bbs"}[5m])
    documentation:
      title: BBS - Lock Held
      description: |
        Whether a BBS instance holds the expected BBS lock (in Locket). 1 means the active BBS server holds the lock, and 0 means the lock was lost.

        Use: This metric is complimentary to Active Locks, and it offers a BBS-level version of the Locket metrics. Although it is emitted per BBS instance, only 1 active lock is held by BBS. Therefore, the expected value is 1. The metric may occasionally be 0 when the BBS instances are performing a leader transition, but a prolonged value of 0 indicates an issue with BBS.

        Origin: Firehose
        Type: Gauge
        Frequency: Periodically
      recommended_alert_thresholds: |
        Red critical: any value other than 1
      recommended_response: |
        1. Run monit status on the instance group that the BBS job is running on to check for failing processes.
        2. If there are no failing processes, then review the logs for BBS.
           - A healthy BBS shows obvious activity around starting or claiming LRPs.
           - An unhealthy BBS leads to the Auctioneer showing minimal or no activity. The BBS sends work to the Auctioneer.

  - name: bbs_lock_held_duration
    promql: avg_over_time(LockHeldDuration{source_id="bbs"}[15m]) / 1000000000
    type: other
    documentation:
      title: BBS - DB Wait Duration
      description: |
        The total time blocked waiting for a new connection.

        Use: If the DB wait duration is going up the state changes in LRPs and Tasks might be failing to be recorded in BBS database. This symptom can also indicate slow connectivity to the BBS database.

        Origin: Firehose
        Type: Gauge (Integer in ns)
        Frequency: 60 s
      recommended_alert_thresholds: |
          Yellow warning: ≥ 10 s
          Red critical: ≥ 20 s
      recommended_response: |
        1. Check BBS logs for errors.
        2. Try vertically scaling the BBS VM resources up. For example, add more CPUs or memory depending on its system.cpu/system.memory metrics.
        3. Check on the connection metrics emitted by the backing database server.
        4. Consider vertically scaling the backing database, if system.cpu and system.memory metrics for the database instances are high.

  - name: bbs_request_latency
    promql: avg_over_time(RequestLatency{source_id="bbs"}[15m]) / 1000000000
    documentation:
      title: BBS - Time to Handle Requests
      description: |
        The maximum observed latency time over the past 60 seconds that the BBS took to handle requests across all its API endpoints.

        Diego is now aggregating this metric to emit the max value observed over 60 seconds.

        Use: If this metric rises, the BBS API is slowing. Response to certain operations is slow if request latency is high.

        Origin: Firehose
        Type: Gauge (Integer in ns)
        Frequency: 60 s
      recommended_alert_thresholds: |
        Yellow warning: ≥ 5s
        Red critical: ≥ 10s
      recommended_response: |
        1. Check CPU and memory statistics.
        2. Check BBS logs for faults and errors that can indicate issues with BBS.
        3. Try scaling the BBS VM resources up. For example, add more CPUs/memory depending on its system.cpu/system.memory metrics.
        4. Consider vertically scaling the backing database, if system.cpu and system.memory metrics for the database instances are high.

